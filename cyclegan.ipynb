{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cyclegan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sean-halpin/cycle_gan_aif1_mouth_only/blob/main/cyclegan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTl9bXsNYo-g"
      },
      "source": [
        "# Data Understanding and Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSvvOf3Lh0OE"
      },
      "source": [
        "Dataset of Smiling & non smiling faces from the [MPLab GENKI Database](https://inc.ucsd.edu/mplab/398.php)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXy99EU7pqc-"
      },
      "source": [
        "### Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghBQJJ61hpBm"
      },
      "source": [
        "import urllib.request\n",
        "url = \"https://inc.ucsd.edu/mplab/databases/GENKI-R2009a.zip\"\n",
        "file_name = \"GENKI.zip\"\n",
        "urllib.request.urlretrieve(url, file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YOpYTUOpudN"
      },
      "source": [
        "### Unzip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jydHmAuGwrtq"
      },
      "source": [
        "import io, zipfile\n",
        "archive = zipfile.ZipFile('GENKI.zip', 'r')\n",
        "archive.extractall(\"dataset/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPxFQvbZsepp"
      },
      "source": [
        "### Read Image Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygETIXSzsd5s"
      },
      "source": [
        "import pandas as pd \n",
        "# From the file dataset/GENKI-R2009a/Subsets/GENKI-4K/README.md , we can see the meaning of the labels\n",
        "# We are interested in if the person is smiling or not\n",
        "data = pd.read_csv(\n",
        "    \"dataset/GENKI-R2009a/Subsets/GENKI-4K/GENKI-4K_Labels.txt\", \n",
        "    sep=\" \", \n",
        "    header=None, \n",
        "    names=[\"smile\", \"pitch\",\"yaw\",\"roll\"],\n",
        "    index_col=None\n",
        ")\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqaX37Kp4Gkn"
      },
      "source": [
        "### Create path to image files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoykhkhOwhUe"
      },
      "source": [
        "image_symlink_location = \"dataset/GENKI-R2009a/Subsets/GENKI-4K/files/\"\n",
        "\n",
        "df1 = data\n",
        "df1['index_col'] = df1.index\n",
        "df1['image'] = df1.apply(lambda x: image_symlink_location + \"file\" + str(int(x['index_col'] + 1)).zfill(4) + \".jpg\", axis=1)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('max_colwidth', None)\n",
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiCZqqYN4L5M"
      },
      "source": [
        "### Preview Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcKJ2DcR4Oce"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "\n",
        "smiles = df1.loc[df1['smile'] == 1]\n",
        "\n",
        "for i,row in smiles.head(2).iterrows():\n",
        "  img = cv2.imread(row.image)\n",
        "  plt.imshow(img)\n",
        "  plt.title(\"Smile \" + str(i))\n",
        "  plt.show() \n",
        "\n",
        "neutral = df1.loc[df1['smile'] == 0]\n",
        "\n",
        "for i,row in neutral.head(2).iterrows():\n",
        "  img = cv2.imread(row.image)\n",
        "  plt.imshow(img)\n",
        "  plt.title(\"Neutral \" + str(i))\n",
        "  plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8awc1pUVjHdx"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ-UVmIxpp9e"
      },
      "source": [
        "### Crop Mouth Only from Images\n",
        "\n",
        "We will crop out the mouth from the images since this will reduce the amount of data fed to the model which is not important to solving our problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqKrD7qLjKsc"
      },
      "source": [
        "# Adapted code from https://www.geeksforgeeks.org/cropping-faces-from-images-using-opencv-python/\n",
        "# Here we use Haarcascades to find the coords of the face within the image, we then display the crop and the full image. \n",
        "# This looks like exactly what we want\n",
        "\n",
        "import cv2\n",
        "  \n",
        "for i,row in df1.head(1).iterrows():\n",
        "  # Read the input image\n",
        "  img = cv2.imread(row.image)\n",
        "  # Convert into grayscale\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  # Load the face detect haar cascade\n",
        "  face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "  # Detect faces\n",
        "  faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "    \n",
        "  # Draw rectangle around the faces and crop the faces\n",
        "  for (x, y, w, h) in faces:\n",
        "    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
        "    faces = img[y:y + h, x:x + w]\n",
        "    plt.imshow(faces)\n",
        "    plt.title(str(i))\n",
        "    plt.show() \n",
        "    # Display the output\n",
        "    plt.imshow(img)\n",
        "    plt.title(str(i))\n",
        "    plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mobn8VuumdY3"
      },
      "source": [
        "### Save Cropped Mouths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYYT9Ttcmg_a"
      },
      "source": [
        "! mkdir -p images_cropped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pKKHW7wm1d6"
      },
      "source": [
        "cropped_image_path = \"images_cropped/\"\n",
        "df = data\n",
        "df['index_col'] = df.index\n",
        "df['image_cropped'] = df.apply(lambda x: cropped_image_path + \"file\" + str(int(x['index_col'] + 1)).zfill(4) + \".jpg\", axis=1)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('max_colwidth', None)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFqyaovWmlgO"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def resize_row(row):\n",
        "  print(row.image)\n",
        "  img = cv2.imread(row.image)\n",
        "  # Convert into grayscale\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  # Load the face detect haar cascade\n",
        "  face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "  # Detect faces\n",
        "  faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "    \n",
        "  # Draw rectangle around the faces and crop the faces\n",
        "  # for (x, y, w, h) in faces:\n",
        "  if len(faces) > 0:\n",
        "    (x, y, w, h) = faces[0]\n",
        "    if h >= 64 and w >= 64:\n",
        "      face = img[y + int(h/2):y + h, x:x + w]\n",
        "      # plt.imshow(face)\n",
        "      # plt.show() \n",
        "      PIL_image = Image.fromarray(np.uint8(face)).convert('RGB')\n",
        "      PIL_image.save(row.image_cropped)      \n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "df['image_crop_valid'] = df.apply(lambda x: resize_row(x), axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o750cwcGWFn"
      },
      "source": [
        "### Cropped Image Resolutions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vSERZQ4GeeN"
      },
      "source": [
        "for i,row in df.head(2).iterrows():\n",
        "  img = cv2.imread(row.image_cropped)\n",
        "  print(\"Image \" + str(i) + \" shape : \" + str(img.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQD0DD4UGyD1"
      },
      "source": [
        "### Problem \n",
        "\n",
        "The images need to be the same resolution to fit the inputs of our model. \n",
        "Lets resize them all. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3dOJdyZHao8"
      },
      "source": [
        "### Image Resizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-ntYNN6Hpnh"
      },
      "source": [
        "! mkdir -p images_resized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxMssQTRHX7F"
      },
      "source": [
        "resized_image_path = \"images_resized/\"\n",
        "image_x = 128\n",
        "image_y = 64\n",
        "\n",
        "df1 = df\n",
        "df1['index_col'] = df1.index\n",
        "df1['image_resized'] = df1.apply(lambda x: resized_image_path + \"file\" + str(int(x['index_col'] + 1)).zfill(4) + \".jpg\", axis=1)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('max_colwidth', None)\n",
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "z0lBBj2EHz8A"
      },
      "source": [
        "from PIL import Image\n",
        "import os, sys\n",
        "\n",
        "def resize():\n",
        "    for i,row in df1.iterrows():\n",
        "        try:\n",
        "            if row.image_crop_valid:\n",
        "              print(row.image_cropped)\n",
        "              if os.path.isfile(row.image_cropped):\n",
        "                  im = Image.open(row.image_cropped)\n",
        "                  f, e = os.path.splitext(row.image_resized)\n",
        "                  imResize = im.resize((image_x,image_y), Image.ANTIALIAS)\n",
        "                  imRGB = imResize.convert(\"RGB\")\n",
        "                  imRGB.save(f + '.jpg', 'JPEG', quality=94)\n",
        "        except:\n",
        "            print(\"Exception while resizing\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "resize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mcpC1sIN2_1"
      },
      "source": [
        "### Preview Resized Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zxu280xLN53m"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "\n",
        "smiles = df1.loc[(df1['smile'] == 1) & (df1['image_crop_valid'] == True)]\n",
        "\n",
        "for i,row in smiles.head(2).iterrows():\n",
        "  img = cv2.imread(row.image_resized)\n",
        "  plt.imshow(img)\n",
        "  plt.title(\"Smile \" + str(i))\n",
        "  plt.show() \n",
        "\n",
        "neutral = df1.loc[(df1['smile'] == 0) & (df1['image_crop_valid'] == True)]\n",
        "\n",
        "for i,row in neutral.head(2).iterrows():\n",
        "  img = cv2.imread(row.image_resized)\n",
        "  plt.imshow(img)\n",
        "  plt.title(\"Neutral \" + str(i))\n",
        "  plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAUJYi7Wrt_3"
      },
      "source": [
        "We may have lost a little quality but these cropped & resized images still look recognizable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIifKjwFYo-u"
      },
      "source": [
        "### Serialize Images as Tensorflow Records\n",
        "\n",
        "The reason we serialize the images as TFRecords is for performance as well as seamless integration with Tensorflow libraries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR8v12sHYo-u"
      },
      "source": [
        "import numpy as np, pandas as pd, os\n",
        "import matplotlib.pyplot as plt, cv2\n",
        "import tensorflow as tf, re, math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baNjYQiYYo-v"
      },
      "source": [
        "print('There are %i neutral images and %i smiles images'%(len(neutral),len(smiles)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPR_cedhYo-v"
      },
      "source": [
        "def _bytes_feature(value):\n",
        "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "  if isinstance(value, type(tf.constant(0))):\n",
        "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _float_feature(value):\n",
        "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "def _int64_feature(value):\n",
        "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uHWn55rYo-w"
      },
      "source": [
        "def serialize_example(feature0, feature1):\n",
        "  feature = {\n",
        "      'image': _bytes_feature(feature0),\n",
        "      'image_name': _bytes_feature(feature1)\n",
        "  }\n",
        "  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "  return example_proto.SerializeToString()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFHPLyt0Yo-w"
      },
      "source": [
        "!mkdir -p neutral_tfrec\n",
        "!mkdir -p smiles_tfrec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spDfhPXtYo-w",
        "scrolled": true
      },
      "source": [
        "SIZE = len(neutral)\n",
        "CT = len(neutral)//SIZE + int(len(neutral)%SIZE!=0)\n",
        "for j in range(CT):\n",
        "    print(); print('Writing TFRecord %i of %i...'%(j,CT))\n",
        "    CT2 = min(SIZE,len(neutral)-j*SIZE)\n",
        "    with tf.io.TFRecordWriter('neutral_tfrec/neutral%.2i-%i.tfrec'%(j,CT2)) as writer:\n",
        "        for k in range(CT2):\n",
        "            # print(neutral.image_resized.iloc[SIZE*j+k])\n",
        "            img = cv2.imread(neutral.image_resized.iloc[SIZE*j+k])\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # Fix incorrect colors\n",
        "            img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, 94))[1].tostring()\n",
        "            name = neutral.image_resized.iloc[SIZE*j+k].split('.')[0]\n",
        "            example = serialize_example(\n",
        "                img, str.encode(name))\n",
        "            writer.write(example)\n",
        "            if k%10==0: print(k,', ',end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8RCl7meYo-x"
      },
      "source": [
        "SIZE = len(smiles)\n",
        "CT = len(smiles)//SIZE + int(len(smiles)%SIZE!=0)\n",
        "for j in range(CT):\n",
        "    print(); print('Writing TFRecord %i of %i...'%(j,CT))\n",
        "    CT2 = min(SIZE,len(smiles)-j*SIZE)\n",
        "    with tf.io.TFRecordWriter('smiles_tfrec/smiles%.2i-%i.tfrec'%(j,CT2)) as writer:\n",
        "        for k in range(CT2):\n",
        "            # print(smiles.image_resized.iloc[SIZE*j+k])\n",
        "            img = cv2.imread(smiles.image_resized.iloc[SIZE*j+k])\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # Fix incorrect colors\n",
        "            img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, 94))[1].tostring()\n",
        "            name = smiles.image_resized.iloc[SIZE*j+k].split('.')[0]\n",
        "            example = serialize_example(\n",
        "                img, str.encode(name))\n",
        "            writer.write(example)\n",
        "            if k%10==0: print(k,', ',end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhA9CUy2Yo-t"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXAmZ37KYo-y"
      },
      "source": [
        "### Prepare Dataset for use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YijeT80_EHB2"
      },
      "source": [
        "!pip install tensorflow_addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nopdeUQQYo-y"
      },
      "source": [
        "# Imports\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg\n",
        "print(tf.__version__)\n",
        "\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "session = InteractiveSession(config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUabdzATYo-z"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_leIz8xYo-z"
      },
      "source": [
        "strategy = tf.distribute.get_strategy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdLapO1-Yo-z"
      },
      "source": [
        "IMAGE_SIZE = [image_y, image_x]\n",
        "\n",
        "def decode_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
        "    return image\n",
        "\n",
        "def read_tfrecord(example):\n",
        "    tfrecord_format = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
        "    image = decode_image(example['image'])\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeQfpdjcYo-0"
      },
      "source": [
        "SMILE_FILENAMES = tf.io.gfile.glob(str('smiles_tfrec/smiles*.tfrec'))\n",
        "print('Smile TFRecord Files:', len(SMILE_FILENAMES))\n",
        "\n",
        "NEUTRAL_FILENAMES = tf.io.gfile.glob(str('neutral_tfrec/neutral*.tfrec'))\n",
        "print('neutral TFRecord Files:', len(NEUTRAL_FILENAMES))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUAqD1_AYo-0"
      },
      "source": [
        "def load_dataset(filenames, labeled=True, ordered=False):\n",
        "    dataset = tf.data.TFRecordDataset(filenames)\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-0mmp44Yo-0"
      },
      "source": [
        "smile_ds = load_dataset(SMILE_FILENAMES, labeled=True).batch(1)\n",
        "neutral_ds = load_dataset(NEUTRAL_FILENAMES, labeled=True).batch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1DFrI56Yo-1"
      },
      "source": [
        "example_smile = next(iter(smile_ds))\n",
        "example_neutral = next(iter(neutral_ds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9TrBTnYYo-1"
      },
      "source": [
        "plt.subplot(121)\n",
        "plt.title('Smile')\n",
        "plt.imshow(example_smile[0] * 0.5 + 0.5)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Neutral')\n",
        "plt.imshow(example_neutral[0] * 0.5 + 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-IQjUm6Yo-2"
      },
      "source": [
        "### Generator "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvaHoqUQYo-2"
      },
      "source": [
        "OUTPUT_CHANNELS = 3\n",
        "\n",
        "def downsample(filters, size, apply_instancenorm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    result = keras.Sequential()\n",
        "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    if apply_instancenorm:\n",
        "        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
        "\n",
        "    result.add(layers.LeakyReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    result = keras.Sequential()\n",
        "    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                      padding='same',\n",
        "                                      kernel_initializer=initializer,\n",
        "                                      use_bias=False))\n",
        "\n",
        "    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
        "\n",
        "    if apply_dropout:\n",
        "        result.add(layers.Dropout(0.4))\n",
        "\n",
        "    result.add(layers.ReLU())\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL009yBKYo-3"
      },
      "source": [
        "def Generator():\n",
        "    inputs = layers.Input(shape=[image_y,image_x,3])\n",
        "\n",
        "    down_stack = [\n",
        "        downsample(64, 4, apply_instancenorm=False), # (bs, 128, 128, 64)\n",
        "        downsample(128, 4), # (bs, 64, 64, 128)\n",
        "        downsample(256, 4), # (bs, 32, 32, 256)\n",
        "        downsample(512, 4), # (bs, 16, 16, 512)\n",
        "        downsample(512, 4), # (bs, 8, 8, 512)\n",
        "        downsample(512, 4), # (bs, 4, 4, 512)\n",
        "        # downsample(512, 4), # (bs, 2, 2, 512)\n",
        "        # downsample(512, 4), # (bs, 1, 1, 512)\n",
        "    ]\n",
        "\n",
        "    up_stack = [\n",
        "        # upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
        "        upsample(512, 4, True), # (bs, 16, 16, 1024)\n",
        "        upsample(256, 4, True), # (bs, 32, 32, 512)\n",
        "        upsample(128, 4), # (bs, 64, 64, 256)\n",
        "        upsample(64, 4), # (bs, 128, 128, 128)\n",
        "    ]\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
        "                                  strides=2,\n",
        "                                  padding='same',\n",
        "                                  kernel_initializer=initializer,\n",
        "                                  activation='tanh') # (bs, 256, 256, 3)\n",
        "\n",
        "    x = inputs\n",
        "\n",
        "    # Downsampling through the model\n",
        "    skips = []\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    # Upsampling and establishing the skip connections\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = layers.Concatenate()([x, skip])\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQguTZ-dYo-3"
      },
      "source": [
        "### Discriminator\n",
        "\n",
        "The discriminator model is a standard convolutional neural network model that takes an image as input and must output a binary classification as to whether it is real or fake. [machinelearningmastery](https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kT5YYPyYo-4"
      },
      "source": [
        "def Discriminator():\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    inp = layers.Input(shape=[image_y,image_x, 3], name='input_image')\n",
        "\n",
        "    x = inp\n",
        "\n",
        "    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
        "    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
        "    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n",
        "\n",
        "    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
        "    conv = layers.Conv2D(512, 4, strides=1,\n",
        "                         kernel_initializer=initializer,\n",
        "                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
        "\n",
        "    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n",
        "\n",
        "    leaky_relu = layers.LeakyReLU()(norm1)\n",
        "\n",
        "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 11, 11, 512)\n",
        "\n",
        "    last = layers.Conv2D(1, 4, strides=1,\n",
        "                         kernel_initializer=initializer)(zero_pad2) # (bs, 8, 8, 1)\n",
        "\n",
        "    return tf.keras.Model(inputs=inp, outputs=last)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNmJvTU2Yo-4"
      },
      "source": [
        "with strategy.scope():\n",
        "    smile_generator = Generator() # transforms neutral to smiling \n",
        "    neutral_generator = Generator() # transforms smiling to nuetral\n",
        "\n",
        "    smile_discriminator = Discriminator() # differentiates real smile and generated smile images\n",
        "    neutral_discriminator = Discriminator() # differentiates real neutral and generated neutral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNFf-UhrYo-4"
      },
      "source": [
        "to_smile = smile_generator(example_neutral)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Original\")\n",
        "plt.imshow(example_neutral[0] * 0.5 + 0.5)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Generated\")\n",
        "plt.imshow(to_smile[0] * 0.5 + 0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxoL7PRrYo-5"
      },
      "source": [
        "### CycleGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz4dzVyrYo-5"
      },
      "source": [
        "class CycleGan(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        smile_generator,\n",
        "        neutral_generator,\n",
        "        smile_discriminator,\n",
        "        neutral_discriminator,\n",
        "        lambda_cycle=10,\n",
        "    ):\n",
        "        super(CycleGan, self).__init__()\n",
        "        self.m_gen = smile_generator\n",
        "        self.p_gen = neutral_generator\n",
        "        self.m_disc = smile_discriminator\n",
        "        self.p_disc = neutral_discriminator\n",
        "        self.lambda_cycle = lambda_cycle\n",
        "        \n",
        "    def compile(\n",
        "        self,\n",
        "        m_gen_optimizer,\n",
        "        p_gen_optimizer,\n",
        "        m_disc_optimizer,\n",
        "        p_disc_optimizer,\n",
        "        gen_loss_fn,\n",
        "        disc_loss_fn,\n",
        "        cycle_loss_fn,\n",
        "        identity_loss_fn\n",
        "    ):\n",
        "        super(CycleGan, self).compile()\n",
        "        self.m_gen_optimizer = m_gen_optimizer\n",
        "        self.p_gen_optimizer = p_gen_optimizer\n",
        "        self.m_disc_optimizer = m_disc_optimizer\n",
        "        self.p_disc_optimizer = p_disc_optimizer\n",
        "        self.gen_loss_fn = gen_loss_fn\n",
        "        self.disc_loss_fn = disc_loss_fn\n",
        "        self.cycle_loss_fn = cycle_loss_fn\n",
        "        self.identity_loss_fn = identity_loss_fn\n",
        "        \n",
        "    def train_step(self, batch_data):\n",
        "        real_smile, real_neutral = batch_data\n",
        "        \n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # neutral to smile back to neutral\n",
        "            fake_smile = self.m_gen(real_neutral, training=True)\n",
        "            cycled_neutral = self.p_gen(fake_smile, training=True)\n",
        "\n",
        "            # smile to neutral back to smile\n",
        "            fake_neutral = self.p_gen(real_smile, training=True)\n",
        "            cycled_smile = self.m_gen(fake_neutral, training=True)\n",
        "\n",
        "            # generating itself\n",
        "            same_smile = self.m_gen(real_smile, training=True)\n",
        "            same_neutral = self.p_gen(real_neutral, training=True)\n",
        "\n",
        "            # discriminator used to check, inputing real images\n",
        "            disc_real_smile = self.m_disc(real_smile, training=True)\n",
        "            disc_real_neutral = self.p_disc(real_neutral, training=True)\n",
        "\n",
        "            # discriminator used to check, inputing fake images\n",
        "            disc_fake_smile = self.m_disc(fake_smile, training=True)\n",
        "            disc_fake_neutral = self.p_disc(fake_neutral, training=True)\n",
        "\n",
        "            # evaluates generator loss\n",
        "            smile_gen_loss = self.gen_loss_fn(disc_fake_smile)\n",
        "            neutral_gen_loss = self.gen_loss_fn(disc_fake_neutral)\n",
        "\n",
        "            # evaluates total cycle consistency loss\n",
        "            total_cycle_loss = self.cycle_loss_fn(real_smile, cycled_smile, self.lambda_cycle) + self.cycle_loss_fn(real_neutral, cycled_neutral, self.lambda_cycle)\n",
        "\n",
        "            # evaluates total generator loss\n",
        "            total_smile_gen_loss = smile_gen_loss + total_cycle_loss + self.identity_loss_fn(real_smile, same_smile, self.lambda_cycle)\n",
        "            total_neutral_gen_loss = neutral_gen_loss + total_cycle_loss + self.identity_loss_fn(real_neutral, same_neutral, self.lambda_cycle)\n",
        "\n",
        "            # evaluates discriminator loss\n",
        "            smile_disc_loss = self.disc_loss_fn(disc_real_smile, disc_fake_smile)\n",
        "            neutral_disc_loss = self.disc_loss_fn(disc_real_neutral, disc_fake_neutral)\n",
        "\n",
        "        # Calculate the gradients for generator and discriminator\n",
        "        smile_generator_gradients = tape.gradient(total_smile_gen_loss,\n",
        "                                                  self.m_gen.trainable_variables)\n",
        "        neutral_generator_gradients = tape.gradient(total_neutral_gen_loss,\n",
        "                                                  self.p_gen.trainable_variables)\n",
        "\n",
        "        smile_discriminator_gradients = tape.gradient(smile_disc_loss,\n",
        "                                                      self.m_disc.trainable_variables)\n",
        "        neutral_discriminator_gradients = tape.gradient(neutral_disc_loss,\n",
        "                                                      self.p_disc.trainable_variables)\n",
        "\n",
        "        # Apply the gradients to the optimizer\n",
        "        self.m_gen_optimizer.apply_gradients(zip(smile_generator_gradients,\n",
        "                                                 self.m_gen.trainable_variables))\n",
        "\n",
        "        self.p_gen_optimizer.apply_gradients(zip(neutral_generator_gradients,\n",
        "                                                 self.p_gen.trainable_variables))\n",
        "\n",
        "        self.m_disc_optimizer.apply_gradients(zip(smile_discriminator_gradients,\n",
        "                                                  self.m_disc.trainable_variables))\n",
        "\n",
        "        self.p_disc_optimizer.apply_gradients(zip(neutral_discriminator_gradients,\n",
        "                                                  self.p_disc.trainable_variables))\n",
        "        \n",
        "        return {\n",
        "            \"smile_gen_loss\": total_smile_gen_loss,\n",
        "            \"neutral_gen_loss\": total_neutral_gen_loss,\n",
        "            \"smile_disc_loss\": smile_disc_loss,\n",
        "            \"neutral_disc_loss\": neutral_disc_loss\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bqCezGCYo-6"
      },
      "source": [
        "with strategy.scope():\n",
        "    def discriminator_loss(real, generated):\n",
        "        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n",
        "\n",
        "        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n",
        "\n",
        "        total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "        return total_disc_loss * 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U8gQVG6Yo-6"
      },
      "source": [
        "with strategy.scope():\n",
        "    def generator_loss(generated):\n",
        "        return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TOpxPTSYo-6"
      },
      "source": [
        "with strategy.scope():\n",
        "    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n",
        "        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
        "\n",
        "        return LAMBDA * loss1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zotlvAV3Yo-6"
      },
      "source": [
        "with strategy.scope():\n",
        "    def identity_loss(real_image, same_image, LAMBDA):\n",
        "        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
        "        return LAMBDA * 0.5 * loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMc8gkVqEyS-"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuhXVyiUgIdh"
      },
      "source": [
        "!mkdir -p models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y03bLhRKj41a"
      },
      "source": [
        "def printSmileFaces(epoch=0,num=5):\n",
        "  _, ax = plt.subplots(num, 2, figsize=(12, 16))\n",
        "  for i, img in enumerate(neutral_ds.take(num)):\n",
        "      prediction = smile_generator(img, training=False)[0].numpy()\n",
        "      prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
        "      img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n",
        "\n",
        "      ax[i, 0].imshow(img)\n",
        "      ax[i, 1].imshow(prediction)\n",
        "      ax[i, 0].set_title(\"Input Photo\")\n",
        "      ax[i, 1].set_title(\"Generated Smile Photo\")\n",
        "      ax[i, 0].axis(\"off\")\n",
        "      ax[i, 1].axis(\"off\")\n",
        "  plt.show()\n",
        "  smile_generator.save(\"models/smile\"+str(epoch)+\".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOn3z4GhYo-8"
      },
      "source": [
        "def printNeutralFaces(epoch=0, num=5):\n",
        "  _, ax = plt.subplots(num, 2, figsize=(12, 16))\n",
        "  for i, img in enumerate(smile_ds.take(num)):\n",
        "      prediction = neutral_generator(img, training=False)[0].numpy()\n",
        "      prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
        "      img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n",
        "\n",
        "      ax[i, 0].imshow(img)\n",
        "      ax[i, 1].imshow(prediction)\n",
        "      ax[i, 0].set_title(\"Input Photo\")\n",
        "      ax[i, 1].set_title(\"Generated Nuetral Photo\")\n",
        "      ax[i, 0].axis(\"off\")\n",
        "      ax[i, 1].axis(\"off\")\n",
        "  plt.show()\n",
        "  neutral_generator.save(\"models/neutral\"+str(epoch)+\".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueRKdpNvFD3T"
      },
      "source": [
        "def make_smile():\n",
        "  for i,row in neutral.head(1).iterrows():\n",
        "    img = cv2.imread(row.image)\n",
        "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    plt.imshow(rgb_img)\n",
        "    plt.title(\"Neutral \" + str(i))\n",
        "    plt.show()\n",
        "    # Load the face detect haar cascade\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    # Detect faces\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "      \n",
        "    # Draw rectangle around the faces and crop the faces\n",
        "    for (x, y, w, h) in faces:\n",
        "    # (x, y, w, h) = faces[0]\n",
        "      # cv2.rectangle(img, (x, y + int(h/2)), (x+w, y+h), (0, 0, 255), 2)\n",
        "      mouth = rgb_img[y + int(h/2):y + h, x:x + w]\n",
        "      plt.imshow(mouth)\n",
        "      plt.title(\"Cropped Mouth \" + str(i))\n",
        "      plt.show()\n",
        "      orig_y,orig_x,d = mouth.shape\n",
        "      resized_mouth = Image.fromarray(mouth).resize((image_x,image_y), Image.ANTIALIAS)\n",
        "      # input = tf.reshape(input, [*IMAGE_SIZE, 3])\n",
        "      input = tf.reshape(tf.keras.preprocessing.image.img_to_array(resized_mouth), [*IMAGE_SIZE, 3])\n",
        "      input = (tf.cast(input, tf.float32) / 127.5) - 1\n",
        "      input = tf.expand_dims(input,axis=0)\n",
        "      prediction = smile_generator(input)[0].numpy()\n",
        "      prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
        "      # Resize to original \n",
        "      print(prediction.shape)\n",
        "      resized_prediction = Image.fromarray(prediction).resize((orig_x,orig_y), Image.ANTIALIAS)\n",
        "      # Display the output\n",
        "      plt.imshow(resized_prediction)\n",
        "      plt.title(\"Generated Smile \" + str(i))\n",
        "      plt.show() \n",
        "      # Add generated mouth back to original image\n",
        "      gen_mouth = np.array(resized_prediction)\n",
        "      rgb_img[y + int(h/2):y + h, x:x + w] = gen_mouth\n",
        "      plt.imshow(rgb_img)\n",
        "      plt.title(\"Image with new Smile \" + str(i))\n",
        "      plt.show()\n",
        "      # print(prediction)\n",
        "      # print(prediction[0] * 0.5 + 0.5)\n",
        "\n",
        "make_smile()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E77IgShSYo-7"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSRIj6zVYo-7"
      },
      "source": [
        "with strategy.scope():\n",
        "    smile_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "    neutral_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "    smile_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "    neutral_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Inuh4XYKYo-7"
      },
      "source": [
        "with strategy.scope():\n",
        "    cycle_gan_model = CycleGan(\n",
        "        smile_generator, neutral_generator, smile_discriminator, neutral_discriminator\n",
        "    )\n",
        "\n",
        "    cycle_gan_model.compile(\n",
        "        m_gen_optimizer = smile_generator_optimizer,\n",
        "        p_gen_optimizer = neutral_generator_optimizer,\n",
        "        m_disc_optimizer = smile_discriminator_optimizer,\n",
        "        p_disc_optimizer = neutral_discriminator_optimizer,\n",
        "        gen_loss_fn = generator_loss,\n",
        "        disc_loss_fn = discriminator_loss,\n",
        "        cycle_loss_fn = calc_cycle_loss,\n",
        "        identity_loss_fn = identity_loss\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tLVr74W1wdy"
      },
      "source": [
        "checkpoint_filepath = 'model_checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_filepath,\n",
        "    verbose=0,\n",
        "    save_best_only=False,\n",
        "    save_weights_only=False,\n",
        "    mode=\"auto\",\n",
        "    save_freq=\"epoch\",\n",
        "    options=None,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQz0j1Ll2Zsp"
      },
      "source": [
        "import os.path\n",
        "from os import path\n",
        "if path.exists(checkpoint_filepath):\n",
        "  cycle_gan_model.load_weights(checkpoint_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0Gpbzb6CrCt"
      },
      "source": [
        "class CustomCallback(keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch % 1 == 0: \n",
        "          printNeutralFaces(epoch, 10)\n",
        "          printSmileFaces(epoch, 10)\n",
        "          make_smile()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0lXgO3DpH_v"
      },
      "source": [
        "printNeutralFaces(0, 10)\n",
        "printSmileFaces(0, 10)\n",
        "make_smile()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiOisAUJ-I19"
      },
      "source": [
        "sample_size = 1500\n",
        "cycle_gan_model.fit(\n",
        "    tf.data.Dataset.zip((smile_ds.take(sample_size), neutral_ds.take(sample_size))),\n",
        "    epochs=4,\n",
        "    callbacks=[model_checkpoint_callback, CustomCallback()]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FmbqTqdBIG6"
      },
      "source": [
        "# Serialize Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXM6j3O1BSMI"
      },
      "source": [
        "\n",
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now() # current date and time\n",
        "date_time = now.strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
        "print(\"date and time:\",date_time)\n",
        "\n",
        "smile_generator.save(\"models/smile_\"+date_time+\".h5\")\n",
        "neutral_generator.save(\"models/neutral_\"+date_time+\".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikKTopjvBxn9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSnr1VG7B2rO"
      },
      "source": [
        "smile_generator.save(\"/content/drive/MyDrive/smile_\"+date_time+\".h5\")\n",
        "neutral_generator.save(\"/content/drive/MyDrive/neutral_\"+date_time+\".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9c5GtEUB7L7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}